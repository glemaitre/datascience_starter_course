{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Depth - Decision Trees and Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll explore a class of algorithms based on decision trees.\n",
    "Decision trees at their root are extremely intuitive.  They\n",
    "encode a series of \"if\" and \"else\" choices, similar to how a person might make a decision.\n",
    "However, which questions to ask, and how to proceed for each answer is entirely learned from the data.\n",
    "\n",
    "For example, if you wanted to create a guide to identifying an animal found in nature, you\n",
    "might ask the following series of questions:\n",
    "\n",
    "- Is the animal bigger or smaller than a meter long?\n",
    "    + *bigger*: does the animal have horns?\n",
    "        - *yes*: are the horns longer than ten centimeters?\n",
    "        - *no*: is the animal wearing a collar\n",
    "    + *smaller*: does the animal have two or four legs?\n",
    "        - *two*: does the animal have wings?\n",
    "        - *four*: does the animal have a bushy tail?\n",
    "\n",
    "and so on.  This binary splitting of questions is the essence of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main benefit of tree-based models is that they require little preprocessing of the data.\n",
    "They can work with variables of different types (continuous and discrete) and are invariant to scaling of the features.\n",
    "\n",
    "Another benefit is that tree-based models are what is called \"nonparametric\", which means they don't have a fix set of parameters to learn. Instead, a tree model can become more and more flexible, if given more data.\n",
    "In other words, the number of free parameters grows with the number of samples and is not fixed, as for example in linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(\n",
    "    centers=[[0, 0], [1, 1]], random_state=61526, n_samples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y)\n",
    "print(f\"The class labels are: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "for klazz, color in zip(classes, [\"tab:blue\", \"tab:red\"]):\n",
    "    mask_sample_klazz = y == klazz\n",
    "    ax.scatter(\n",
    "        X[mask_sample_klazz, 0], X[mask_sample_klazz, 1],\n",
    "        color=color, label=klazz,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "plt.axis(\"square\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Feature #0\")\n",
    "_ = plt.ylabel(\"Feature #1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function to create this scatter plot by passing 2 variables: `data` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, labels, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    classes = np.unique(labels)\n",
    "    for klazz, color in zip(classes, [\"tab:blue\", \"tab:red\"]):\n",
    "        mask_sample_klazz = labels == klazz\n",
    "        ax.scatter(\n",
    "            data[mask_sample_klazz, 0], data[mask_sample_klazz, 1],\n",
    "            color=color, label=klazz,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "    sns.despine()\n",
    "    ax.axis(\"square\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Feature #0\")\n",
    "    _ = plt.ylabel(\"Feature #1\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn a set of binary rule using a portion of the data. Using the rules learned, we will predict on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the decision boundaries found using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import DecisionBoundaryDisplay\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf, X, cmap=plt.cm.RdBu_r, alpha=0.5\n",
    ")\n",
    "_ = plot_data(X_train, y_train, ax=display.ax_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we get the following classification on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf, X, cmap=plt.cm.RdBu_r, alpha=0.5\n",
    ")\n",
    "_ = plot_data(X_test, y_test, ax=display.ax_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "     <li> Modify the depth of the tree and see how the partitioning evolves. </li>\n",
    "     <li>What can you say about under- and over-fitting of the tree model?</li>\n",
    "     <li>How would you choose the best depth?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many parameter that control the complexity of a tree, but the one that might be easiest to understand is the maximum depth. This limits how finely the tree can partition the input space, or how many \"if-else\" questions can be asked before deciding which class a sample lies in.\n",
    "\n",
    "This parameter is important to tune for trees and tree-based models. The interactive plot below shows how underfit and overfit looks like for this model. Having a ``max_depth`` of 1 is clearly an underfit model, while a depth of 7 or 8 clearly overfits. The maximum depth a tree can be grown at for this dataset is 8, at which point each leave only contains samples from a single class. This is known as all leaves being \"pure.\"\n",
    "\n",
    "In the interactive plot below, the regions are assigned blue and red colors to indicate the predicted class for that region. The shade of the color indicates the predicted probability for that class (darker = higher probability), while yellow regions indicate an equal predicted probability for either class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import plot_tree_interactive\n",
    "plot_tree_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside note regarding the partitioning in decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will go slightly more into details regading how a tree is selecting the best partition. First, instead of using synthetic data, we will use a real dataset this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets/penguins.csv\")\n",
    "dataset = dataset.dropna(subset=[\"Body Mass (g)\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a decision tree to classify the penguin species using their body mass as a feature. To simplify the problem will focus only the Adelie and Gentoo species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the column of interest\n",
    "dataset = dataset[[\"Body Mass (g)\", \"Species\"]]\n",
    "# Make the species name more readable\n",
    "dataset[\"Species\"] = dataset[\"Species\"].apply(lambda x: x.split()[0])\n",
    "# Only select the Adelie and Gentoo penguins\n",
    "dataset = dataset.set_index(\"Species\").loc[[\"Adelie\", \"Gentoo\"], :]\n",
    "# Sort all penguins by their body mass\n",
    "dataset = dataset.sort_values(by=\"Body Mass (g)\")\n",
    "# Convert the dataframe (2D) to a series (1D)\n",
    "dataset = dataset.squeeze()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look at the body mass distribution for each specie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "dataset.groupby(\"Species\").hist(ax=ax, alpha=0.7, legend=True, density=True)\n",
    "ax.set_ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead to look at the distribution, we can look at all samples directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "_ = ax.set_xlabel(dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we build a tree, we want to find splits, one at the time, such that we partition the data in way that classes as \"unmixed\" as possible. Let's make a first completely random split to highlight the principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random state such we all have the same results\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = rng.choice(dataset.size)\n",
    "\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.set_xlabel(dataset.name)\n",
    "ax.set_title(f\"Body mass threshold: {dataset[random_idx]} grams\")\n",
    "_ = ax.vlines(dataset[random_idx], -1, 1, color=\"red\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the split done, we seek for having two partitions for which the samples are as much as possible from a single class and contains as many samples as possible. In decision tree, we used a **criterion** to assess the quality of a split. The **entropy** is one of the statistic which can describe the class mixity in a partition. Let's compute the entropy for the full dataset, the set on the left of the threshold and the set on the right of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_entropy = entropy(\n",
    "    dataset.index.value_counts(normalize=True)\n",
    ")\n",
    "parent_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_entropy = entropy(\n",
    "    dataset[:random_idx].index.value_counts(normalize=True)\n",
    ")\n",
    "left_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_entropy = entropy(\n",
    "    dataset[random_idx:].index.value_counts(normalize=True)\n",
    ")\n",
    "right_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the quality of the split by combining the entropies. This is known as the **information gain**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_entropy - (left_entropy + right_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we should normalize the entropies with the number of samples in each sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels_parent, labels_left, labels_right):\n",
    "    # compute the entropies\n",
    "    entropy_parent = entropy(labels_parent.value_counts(normalize=True))\n",
    "    entropy_left = entropy(labels_left.value_counts(normalize=True))\n",
    "    entropy_right = entropy(labels_right.value_counts(normalize=True))\n",
    "\n",
    "    n_samples_parent = labels_parent.size\n",
    "    n_samples_left = labels_left.size\n",
    "    n_samples_right = labels_right.size\n",
    "\n",
    "    # normalize with the number of samples\n",
    "    normalized_entropy_left = ((n_samples_left / n_samples_parent) * \n",
    "                               entropy_left)\n",
    "    normalized_entropy_right = ((n_samples_right / n_samples_parent) *\n",
    "                                entropy_right)\n",
    "\n",
    "    return (entropy_parent -\n",
    "            normalized_entropy_left - normalized_entropy_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(\n",
    "    dataset.index,\n",
    "    dataset[:random_idx].index,\n",
    "    dataset[random_idx:].index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can compute the information gain for all possible body mass thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain = pd.Series(\n",
    "    [information_gain(dataset.index, dataset[:idx].index, dataset[idx:].index)\n",
    "     for idx in range(dataset.size)],\n",
    "    index=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = all_information_gain.plot()\n",
    "_ = ax.set_ylabel(\"Information gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the maximum of the information gain corresponds to the split which best partition our data. So we can check the corresponding body mass threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.vlines(\n",
    "    all_information_gain.idxmax(), -1, 1,\n",
    "    color=\"red\", linestyle=\"--\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(42)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y_no_noise = np.sin(4 * x) + x\n",
    "y = y_no_noise + rnd.normal(size=len(x))\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "_ = plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=2)\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-3, 3, 1000).reshape((-1, 1))\n",
    "y_test = reg.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_test.ravel(), y_test, color='tab:blue', label=\"prediction\")\n",
    "plt.plot(X.ravel(), y, 'C7.', label=\"training data\")\n",
    "_ = plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree allows us to estimate the signal in a non-parametric way,\n",
    "but clearly has some issues.  In some regions, the model shows high bias and\n",
    "under-fits the data.\n",
    "(seen in the long flat lines which don't follow the contours of the data),\n",
    "while in other regions the model shows high variance and over-fits the data\n",
    "(reflected in the narrow spikes which are influenced by noise in single points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Take the above example and repeat the training/testing by changing depth of the tree.\n",
    "      </li>\n",
    "      <li>\n",
    "      What can you conclude?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that by increasing the depth of the tree, we are going to get an over-fitted model. A way to bypass the choice of a specific depth it to combine several trees together.\n",
    "\n",
    "Let's start by training several trees on slightly different data. The slightly different dataset could be generated by randomly sampling with replacement. In statistics, this called a boostrap sample. We will use the iris dataset to create such ensemble and ensure that we have some data for training and some left out data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to train several decision trees, we will run a single tree. However, instead to train this tree on `X_train`, we want to train it on a bootstrap sample. You can use the `np.random.choice` function sample with replacement some index. You will need to create a sample_weight vector and pass it to the `fit` method of the `DecisionTreeClassifier`. We provide the `generate_sample_weight` function which will generate the `sample_weight` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_idx(X):\n",
    "    indices = np.random.choice(\n",
    "        np.arange(X.shape[0]), size=X.shape[0], replace=True\n",
    "    )\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_idx(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(bootstrap_idx(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    indices = bootstrap_idx(X)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bootstrap, y_train_bootstrap = bootstrap_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classes distribution in the original data: {Counter(y_train)}')\n",
    "print(f'Classes distribution in the bootstrap: {Counter(y_train_bootstrap)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Create a bagging classifier</b>:\n",
    "    <br>\n",
    "    A bagging classifier will train several decision tree classifiers, each of them on a different bootstrap sample.\n",
    "     <ul>\n",
    "      <li>\n",
    "      Create several `DecisionTreeClassifier` and store them in a Python list;\n",
    "      </li>\n",
    "      <li>\n",
    "      Loop over these trees and `fit` them by generating a bootstrap sample using `bootstrap_sample` function;\n",
    "      </li>\n",
    "      <li>\n",
    "      To predict with this ensemble of trees on new data (testing set), you can provide the same set to each tree and call the `predict` method. Aggregate all predictions in a NumPy array;\n",
    "      </li>\n",
    "      <li>\n",
    "      Once the predictions available, you need to provide a single prediction: you can retain the class which was the most predicted which is called a majority vote;\n",
    "      </li>\n",
    "      <li>\n",
    "      Finally, check the accuracy of your model.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: using scikit-learn</b>:\n",
    "    <br>\n",
    "    After implementing your own bagging classifier, use a `BaggingClassifier` from scikit-learn to fit the above data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very famous classifier is the random forest classifier. It is similar to the bagging classifier. In addition of the bootstrap, the random forest will use a subset of features (selected randomly) to find the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Create a random forest classifier</b>:\n",
    "    <br>\n",
    "    Use your previous code which was generated several `DecisionTreeClassifier`. Check the list of the option of this classifier and modify one of the parameters such that only the $\\sqrt{F}$ features are used for the splitting. $F$ represents the number of features in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: using scikit-learn</b>:\n",
    "    <br>\n",
    "    After implementing your own random forest classifier, use a `RandomForestClassifier` from scikit-learn to fit the above data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import plot_forest_interactive\n",
    "plot_forest_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another option: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Ensemble method that can be useful is *Boosting*: here, rather than\n",
    "looking at 200 (say) parallel estimators, we construct a chain of 200 estimators\n",
    "which iteratively refine the results of the previous estimator.\n",
    "The idea is that by sequentially applying very fast, simple models, we can get a\n",
    "total model error which is better than any of the individual pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=.2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>ACCELERATE GRADIENT BOOSTING</b>:\n",
    "    <ul>\n",
    "        <li>Which solution would you use to accelerate the training speed of gradient boosting algorithm.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides `HistGradientBoostingClassifier` which is an approximate gradient boosting algorithm similar to `lightgbm` and `xgboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf = HistGradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
