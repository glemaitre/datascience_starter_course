{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82622657",
   "metadata": {},
   "source": [
    "# Linear models\n",
    "\n",
    "In this notebook, we introduce in depth a family of machine learning model called linear models.\n",
    "\n",
    "## Which part of the model is linear?\n",
    "\n",
    "As we previously discussed, a predictive model is a mathematical function $f$ that given a matrix of feature $X$ provides some predictions $\\hat{y}$. Formally, we have $\\hat{y}= f(X)$. Linear models define a certain type of functions $f$:\n",
    "\n",
    "$$\n",
    "f{X} = \\beta X \\\\\n",
    "f{X} = a_0 + a_1 X_1 + a_2 X_2 + \\dots + a_n X_n\n",
    "$$\n",
    "\n",
    "In other words, a linear model is a function that linearly combined the features of the matrix $X$ to provide a prediction $y$. Let's take a simple regression example with a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"datasets/penguins_regression.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343c06d",
   "metadata": {},
   "source": [
    "In this dataset, we want to use a linear model that given the flipper length of a penguin, we predict the body mass of the penguin. First, let's have a look at the relationship between these two measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(data[\"Flipper Length (mm)\"], data[\"Body Mass (g)\"])\n",
    "ax.set_xlabel(\"Flipper length (mm)\")\n",
    "ax.set_ylabel(\"Body mass (g)\")\n",
    "_ = ax.set_title(\n",
    "    \"Body mass of a penguin in function of its flipper length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7fdcd",
   "metadata": {},
   "source": [
    "We observe that we have a kind of linear relationship: longer is the flipper, heavier is the penguin. A linear model in this context, would be a function that is parametrized to provide a straight line. We can define a function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43922fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, coef, intercept):\n",
    "    predictions = coef * x + intercept\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5eef3",
   "metadata": {},
   "source": [
    "This function take as input `x` that corresponds to the flipper length and is then parametrize by `coef` and `intercep`. Those correspond to the $a_0$ and $a_1$ of the equation of the linear model given above.\n",
    "\n",
    "Let's make a try by defining a value for these two parameter and observe the resulting line on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b163032",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 0\n",
    "coef = 20\n",
    "predicted_body_mass = f(data[\"Flipper Length (mm)\"], coef, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(data[\"Flipper Length (mm)\"], data[\"Body Mass (g)\"])\n",
    "ax.plot(\n",
    "    data[\"Flipper Length (mm)\"],\n",
    "    predicted_body_mass,\n",
    "    linewidth=3,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Predictive model\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Flipper length (mm)\")\n",
    "ax.set_ylabel(\"Body mass (g)\")\n",
    "_ = ax.set_title(\n",
    "    \"Body mass of a penguin in function of its flipper length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74e4b4",
   "metadata": {},
   "source": [
    "We just built our linear predictive model: passing a `x` value will provide us a body mass prediction.\n",
    "\n",
    "### Question\n",
    "\n",
    "- *Given this model, how would you quantified the quality of this predictive model?*\n",
    "- *Can you provide a set of parameters for which the predictions are more accurate?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0739116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "mean_absolute_error(data[\"Body Mass (g)\"], predicted_body_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "mean_squared_error(data[\"Body Mass (g)\"], predicted_body_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54f33d",
   "metadata": {},
   "source": [
    "## From manual to automatized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96516abe",
   "metadata": {},
   "source": [
    "In the previous exercise, we define some \"metric\" that define the quality of a predictive model. We could use such a metric to find the optimal predicitive model; the model for which the error is minimum. This metric is also known as **loss function**.\n",
    "\n",
    "A bit of numerical optimization and applied mathematics tell us that we can try to use gradient descent algorithm and mathematical derivative to find the minimum of the function. But we will just rely on SciPy that implement such algorithm for us. We need to modify a bit the previous error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_mean_squared_error(params, data):\n",
    "    y_pred = f(\n",
    "        data[\"Flipper Length (mm)\"],\n",
    "        coef=params[1],\n",
    "        intercept=params[0],\n",
    "    )\n",
    "    return mean_squared_error(data[\"Body Mass (g)\"], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c06377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "results = minimize(\n",
    "    f_mean_squared_error, x0=(intercept, coef), args=data\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ff62b",
   "metadata": {},
   "source": [
    "So the function `minimize` find a set of parameters with the lowest possible error. We can check the value of the intercept and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec92f53",
   "metadata": {},
   "source": [
    "Let's use these parameters to check visually what is the output of such parametric model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c28a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(data[\"Flipper Length (mm)\"], data[\"Body Mass (g)\"])\n",
    "ax.plot(\n",
    "    data[\"Flipper Length (mm)\"],\n",
    "    f(data[\"Flipper Length (mm)\"], coef=results.x[1], intercept=results.x[0]),\n",
    "    linewidth=3,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Predictive model\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Flipper length (mm)\")\n",
    "ax.set_ylabel(\"Body mass (g)\")\n",
    "_ = ax.set_title(\n",
    "    \"Body mass of a penguin in function of its flipper length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316a32d",
   "metadata": {},
   "source": [
    "We see that this model is much better than our manually defined model. This model is the one minimizing the average of the squared errors.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- *Use `sklearn.linear_model.LinearRegression` to `fit` a model.*\n",
    "- *By looking a the documentation, what are the value of the coefficients of the linear model?*\n",
    "- *Plot the predictions that you can obtain with `predict` as in the previous plot.*\n",
    "- *Use `sklearn.metrics.mean_squared_error` to compute the error of this model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ee07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(data[[\"Flipper Length (mm)\"]], data[\"Body Mass (g)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7132fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(data[[\"Flipper Length (mm)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(data[\"Body Mass (g)\"], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481053af",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(data[\"Flipper Length (mm)\"], data[\"Body Mass (g)\"])\n",
    "ax.plot(\n",
    "    data[\"Flipper Length (mm)\"],\n",
    "    y_pred,\n",
    "    linewidth=3,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Predictive model\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Flipper length (mm)\")\n",
    "ax.set_ylabel(\"Body mass (g)\")\n",
    "_ = ax.set_title(\n",
    "    \"Body mass of a penguin in function of its flipper length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02758596",
   "metadata": {},
   "source": [
    "## What about other loss functions?\n",
    "\n",
    "So `LinearRegression` minimized the mean squared error. But we might want to minize the absolute error instead. But first let's check what is the different between the squared error and absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e58dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-2, 2, num=100)\n",
    "\n",
    "plt.plot(xx, (xx - 0) ** 2, label=\"squared error\")\n",
    "plt.plot(xx, np.abs(xx - 0), label=\"absolute error\")\n",
    "plt.ylabel(\"Error\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93126cdc",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- *What is the difference that you can observe between the two type of error?*\n",
    "- *What is the practical implications?*\n",
    "\n",
    "Let's go in a situation that a scientist made measurements of penguins but \"dg\" instead of \"g\". We will create a new dataframe containing the new measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fake_penguins = 50\n",
    "fake_flipper_length = np.random.uniform(low=220, high=230, size=n_fake_penguins)\n",
    "fake_body_mass = np.random.uniform(low=550, high=650, size=n_fake_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60629661",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat(\n",
    "    [\n",
    "        data,\n",
    "        pd.DataFrame({\n",
    "            \"Flipper Length (mm)\": fake_flipper_length,\n",
    "            \"Body Mass (g)\": fake_body_mass\n",
    "        })\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa4ff2",
   "metadata": {},
   "source": [
    "We can quickly plot the dataset to have an idea on the impact of the error commited by the scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c662a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = new_data.plot.scatter(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d977da7",
   "metadata": {},
   "source": [
    "So we observe those new data samples that could be consider as \"ouliers\".\n",
    "\n",
    "### Questions\n",
    "\n",
    "- *Fit a `LinearRegression` model on this new dataset.*\n",
    "- *Are the coefficients significantly different from the previous model?*\n",
    "- *Display the predictions of the model and compare it with our previous model. Do you observe any difference?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caff17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(new_data[[\"Flipper Length (mm)\"]], new_data[\"Body Mass (g)\"])\n",
    "y_pred = model.predict(new_data[[\"Flipper Length (mm)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c34104",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(new_data[\"Flipper Length (mm)\"], new_data[\"Body Mass (g)\"])\n",
    "ax.plot(\n",
    "    new_data[\"Flipper Length (mm)\"],\n",
    "    y_pred,\n",
    "    linewidth=3,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Predictive model\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Flipper length (mm)\")\n",
    "ax.set_ylabel(\"Body mass (g)\")\n",
    "_ = ax.set_title(\n",
    "    \"Body mass of a penguin in function of its flipper length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88beb7f3",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- *Repeat the previous experiment and fit a `sklearn.linear_model.QuantileRegressor`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "model = QuantileRegressor(solver=\"highs\")\n",
    "model.fit(new_data[[\"Flipper Length (mm)\"]], new_data[\"Body Mass (g)\"])\n",
    "y_pred = model.predict(new_data[[\"Flipper Length (mm)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78547e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(new_data[\"Flipper Length (mm)\"], new_data[\"Body Mass (g)\"])\n",
    "ax.plot(\n",
    "    new_data[\"Flipper Length (mm)\"],\n",
    "    y_pred,\n",
    "    linewidth=3,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"Predictive model\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Flipper length (mm)\")\n",
    "ax.set_ylabel(\"Body mass (g)\")\n",
    "_ = ax.set_title(\n",
    "    \"Body mass of a penguin in function of its flipper length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc44b8c",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- *Create two new models where you should get an estimator for the 10th and 90th quantiles instead of the default median. You can check the parameter `quantile` of the `QuantileRegressor`.*\n",
    "- *Plot the the predictions that you get with the 10th, 50th, and 90th quantiles. What the 10-90 quantiles coverage represent?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a64f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuantileRegressor(solver=\"highs\", quantile=0.1)\n",
    "model.fit(\n",
    "    new_data[[\"Flipper Length (mm)\"]], new_data[\"Body Mass (g)\"]\n",
    ")\n",
    "y_pred_10 = model.predict(new_data[[\"Flipper Length (mm)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460da7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuantileRegressor(solver=\"highs\", quantile=0.9)\n",
    "model.fit(\n",
    "    new_data[[\"Flipper Length (mm)\"]], new_data[\"Body Mass (g)\"]\n",
    ")\n",
    "y_pred_90 = model.predict(new_data[[\"Flipper Length (mm)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(new_data[\"Flipper Length (mm)\"], new_data[\"Body Mass (g)\"])\n",
    "ax.plot(\n",
    "    new_data[\"Flipper Length (mm)\"],\n",
    "    y_pred,\n",
    "    linewidth=3,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"quantile=0.5\",\n",
    ")\n",
    "ax.plot(\n",
    "    new_data[\"Flipper Length (mm)\"],\n",
    "    y_pred_10,\n",
    "    linewidth=3,\n",
    "    color=\"tab:green\",\n",
    "    label=\"quantile=0.1\",\n",
    ")\n",
    "ax.plot(\n",
    "    new_data[\"Flipper Length (mm)\"],\n",
    "    y_pred_90,\n",
    "    linewidth=3,\n",
    "    color=\"tab:red\",\n",
    "    label=\"quantile=0.9\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Flipper length (mm)\")\n",
    "ax.set_ylabel(\"Body mass (g)\")\n",
    "_ = ax.set_title(\n",
    "    \"Body mass of a penguin in function of its flipper length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a81656",
   "metadata": {},
   "source": [
    "## What about multiple features in `X`?\n",
    "\n",
    "Up to now, we saw a use case where we dealt with a single feature and a single target. The previous experiment can be extended to multiple features. Let's create a (fake) feature called \"Flipper Width\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Flipper Width (mm)\"] = data[\"Flipper Length (mm)\"] / 10 + np.random.randn(len(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cc03c",
   "metadata": {},
   "source": [
    "It is a bit more complex but we can still plot the data in a 3-dimensional scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(\n",
    "    data[\"Flipper Length (mm)\"],\n",
    "    data[\"Flipper Width (mm)\"],\n",
    "    data[\"Body Mass (g)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae59f39",
   "metadata": {},
   "source": [
    "We will use `LinearRegression` and use the 2 flipper measurements as input data and predict the body mass as previously done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(\n",
    "    data[[\"Flipper Length (mm)\", \"Flipper Width (mm)\"]],\n",
    "    data[\"Body Mass (g)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f46c0",
   "metadata": {},
   "source": [
    "We can check the coefficients and intercept to check the difference compare to our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8629fd",
   "metadata": {},
   "source": [
    "We observe that we have an additional coefficient corresponding to a weight associated with the new \"Flipper Width\". We can now plot the predictions obtained with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(\n",
    "    data[\"Flipper Length (mm)\"], data[\"Flipper Width (mm)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(np.vstack([xx.ravel(), yy.ravel()]).T).reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3424d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(\n",
    "    data[\"Flipper Length (mm)\"],\n",
    "    data[\"Flipper Width (mm)\"],\n",
    "    data[\"Body Mass (g)\"],\n",
    ")\n",
    "_ = ax.plot_surface(xx, yy, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52861ef1",
   "metadata": {},
   "source": [
    "In this case, we observe that the straight line in the 1-D case become a plane in the 2-D case.\n",
    "\n",
    "## Feature engineering or getting a non-linear decision function\n",
    "\n",
    "Up to now, we saw that the predictions boil down to a linear combination of the input feature. Sometimes, this could be limiting when there is a non-linear relationship between the input features and the target. Let's generate some synthetic data where this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd41d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "X = np.linspace(-5, 5, n_samples)\n",
    "y = X + 2 * np.cos(2 * np.pi * X) + 2 * np.random.rand(n_samples)\n",
    "X = X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521382ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b230542",
   "metadata": {},
   "source": [
    "We generated true target as a combination of a linear trend with and additional periodic signal and a bit of noise.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- *Fit a `LinearRegression` model.*\n",
    "- *Show the predictions obtained with this model.*\n",
    "- *Add a new column in `X` that corresponds to `np.cos(2 * np.pi * X[:, 0]`*\n",
    "- *Fit again a `LinearRegression` model and display the prediction. Is the decision function linear?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a44d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90593382",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), linewidth=3, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f915ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(\n",
    "    [X, np.cos(2 * np.pi* X)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X, y)\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(X[:, 0], model.predict(X), linewidth=3, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c65f58",
   "metadata": {},
   "source": [
    "## Feature engineering, overfitting, and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04316491",
   "metadata": {},
   "source": [
    "We might not know in advance what the period of the signal. So instead, we could add several columns in `X` and vary the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eeb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "X = np.linspace(-5, 5, n_samples)\n",
    "y = X + 2 * np.cos(2 * np.pi * X) + 2 * np.random.rand(n_samples)\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "n_periods = 10\n",
    "for i in range(1, n_periods):\n",
    "    X = np.concatenate(\n",
    "        [X, np.cos(i * np.pi* X[:, [0]])], axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X, y)\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(X[:, 0], model.predict(X), linewidth=3, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7591e",
   "metadata": {},
   "source": [
    "We see that in this case, our model pick up the right trend.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- *Repeat the same experiment by increasing the number of columns, let say 1,000 to have a large range of cosine periods.*\n",
    "- *What do you observe?*\n",
    "- *Instead of a `LinearRegression`, fit a `sklearn.linear_model.Ridge` model with the default parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge().fit(X, y)\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(X[:, 0], model.predict(X), linewidth=3, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a4662",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- *Vary the `alpha` parameter (from 1e-6 to 1e3) of the `Ridge` model.*\n",
    "- *Check the predictions that you obtain. What is the role of the `alpha` parmameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1e3).fit(X, y)\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(X[:, 0], model.predict(X), linewidth=3, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1e-6).fit(X, y)\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(X[:, 0], model.predict(X), linewidth=3, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec905b2c",
   "metadata": {},
   "source": [
    "### Questions?\n",
    "\n",
    "- *Repeat the same experiment using a `sklearn.linear_model.Lasso` model. You can have a look at the `coef_` attribute as well to get more insights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=1e-1).fit(X, y)\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.plot(X[:, 0], model.predict(X), linewidth=3, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbff73",
   "metadata": {},
   "source": [
    "## From regression to classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1580b8",
   "metadata": {},
   "source": [
    "Up to now, we focus on a regression problem. However, the above technique is not intended to be used for classification. Let's use a classification dataset to see the difference with the regression that we observe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"datasets/penguins_classification.csv\")\n",
    "X = data[[\"Culmen Length (mm)\"]]\n",
    "y = (data[\"Species\"] == \"Adelie\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c260f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.ylabel(\"Is Adelie penguin?\")\n",
    "_ = plt.xlabel(\"Culmen Length (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1601f7",
   "metadata": {},
   "source": [
    "We see that the target takes only the value 0 or 1. Indeed, we could use the sigmoid function in order to have constraint the value of any real number to be in the range [0, 1]. This is called a `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(X, y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label=\"truth\")\n",
    "plt.scatter(X, y_pred, label=\"predictions\")\n",
    "plt.ylabel(\"Is Adelie penguin?\")\n",
    "plt.xlabel(\"Culmen Length (mm)\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749cbb91",
   "metadata": {},
   "source": [
    "So it means that in practice, the classification give \"hard\" prediction that should be 0 or 1). However, the sigmoid function can provide continuous values that relates to the probability to be the class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6918208",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict_proba(X)\n",
    "y_proba[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b469f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label=\"truth\")\n",
    "plt.scatter(X, y_proba[:, 1], label=\"predictions\")\n",
    "plt.ylabel(\"Is Adelie penguin?\")\n",
    "plt.xlabel(\"Culmen Length (mm)\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6258b",
   "metadata": {},
   "source": [
    "## The importance of preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b459e33",
   "metadata": {},
   "source": [
    "For the moment, we use only a regression or classification model without doing any preprocessing on the data. However, it could be an issue. Let's dive into this.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- *Load the `sklearn.datasets.load_iris` datasets.*\n",
    "- *Fit a `LogisticRegression` model.*\n",
    "- *Do you observe any issue?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf692ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8fd45",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- *Increase the number of iterations `max_iter` and check if it solves the problem.*\n",
    "- *Is there alternative solution that could be better?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac24d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1_000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef546023",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalize = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_normalize, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a881ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa02d6",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- *Instead of the manual scaling above, use the `sklearn.preprocessing.StandardScaler` model.*\n",
    "- *Call `fit` and `transform` methods on the data to see the effect.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a522bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a52f8",
   "metadata": {},
   "source": [
    "Now, let's imagine that you have a training and testing set.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- *On which dataset should I call `fit`?*\n",
    "- *On which dataset should I call `transform`?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda65dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb403f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)\n",
    "X_train_normalize = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc046d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalize = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440824d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_normalize, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, model.predict(X_test_normalize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f91bf3",
   "metadata": {},
   "source": [
    "## The scikit-learn `Pipeline`\n",
    "\n",
    "\n",
    "To simplify the processing and to not make mistake regarding when calling `fit` and `transform`, one can use `Pipeline`. A pipeline is just a chain of steps that would transform the data with a final steps that is usually a regressor or a classifier. The way to use the pipeline is identical of using the last steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_model\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d549c83",
   "metadata": {},
   "source": [
    "When fitting the model, internally we call `fit` and `transform` of the `StandardScaler` and the `fit` method of the `LogsiticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c391ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3598f",
   "metadata": {},
   "source": [
    "We can access a pipeline step using the Python indexing. For instance, we can get the coefficient of the `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29e4b0",
   "metadata": {},
   "source": [
    "When calling `predict`, it will be equivalent to call `transform` of the `StandardScaler` followed by the `predict` method of the `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1490110",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5e54d",
   "metadata": {},
   "source": [
    "## Dealing with categorical data\n",
    "\n",
    "Up to now, we used data that were only numerical values. This is quite restrictive and many datasets come with some type of data that are called categorical data. We can recognize a categorical feature because the feature values are limited to a small cardinality of choices (e.g. countries, sex, etc.).\n",
    "\n",
    "There are two questions to have in mind when it comes to deal with these specific type of data:\n",
    "\n",
    "- How to handle to non-numeric data?\n",
    "- What is the impact of the type of encoding on the underlying model?\n",
    "\n",
    "Let's first have a try on a more real dataset that will contain heterogeneous types (numerical and categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[\"class\"])\n",
    "y = data[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4922478",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- *Fit a `LogisticRegression` model on the training set.*\n",
    "- *What do you get?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b94c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd894c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d45bda",
   "metadata": {},
   "source": [
    "### Encoding categories with a specific order\n",
    "\n",
    "The simplest way that one can think of is to replace a category by a numerical value. This is indeed the job of the `sklearn.preprocessing.OrdinalEncoder`. Let's give an example of the output such model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31791c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native-country\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder().fit(X[categorical_features])\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.transform(X[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c904383",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- *What is the impact of using such encoding on the modelling used by a linear model?*\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "Another solution and more desired with linear model is to create a matrix of zeros, with the number of columns corresponding to a category. When the feature value corresponds to a category, then the entry is labelled with a one (and so the name one-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1daafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(X[categorical_features])\n",
    "encoder.transform(X[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6041d5",
   "metadata": {},
   "source": [
    "## Dealing with heterogeneous data in a single predictive pipeline\n",
    "\n",
    "Now that we know how to deal with numerical and categorical data, we show how to use `ColumnTransformer` that is a transformer allowing to apply a specific transformer (or pipeline of transformers) to a list of columns.\n",
    "\n",
    "The `ColumnTransformer` should be think as a 3-steps procedure: split columns by group (provided by the user), transform each individual group of columns, and combine the resulting transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_column = [\n",
    "    \"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"\n",
    "]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_preprocessor\", OneHotEncoder(), categorical_features),\n",
    "        (\"num_preprocessor\", StandardScaler(), numerical_column)\n",
    "    ]\n",
    ")\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"linear_model\", LogisticRegression(max_iter=1_000)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb646b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[:-1].transform(X_train).A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
