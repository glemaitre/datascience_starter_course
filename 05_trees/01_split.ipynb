{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From a single split to a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we study the internal mechanism used by decision tree to make some classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a dataset of penguins. Let's first load the dataset and quickly check what type of data do we have to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"datasets/penguins.csv\").dropna(subset=[\"Body Mass (g)\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will frame our supervised learning problem in the following manner: we are interested to predict the penguin species with a single feature, namely the \"Body Mass\". Then, we will select these data of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"Body Mass (g)\"].to_numpy()\n",
    "y = dataset[\"Species\"].apply(lambda x: x.split()[0]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease some later processing, we are sorting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sorted_idx = np.argsort(X)\n",
    "X = X[sorted_idx]\n",
    "y = y[sorted_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give a look at the class distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for clazz in np.unique(y):\n",
    "    plt.hist(X[y==clazz], alpha=0.7, label=f'{clazz}', density=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Body mass (g)')\n",
    "plt.ylabel('Class probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have only few samples, we can use a `swarmplot` to visualize all samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame({'Body mass (g)': X, 'label': y, '': \"\"})\n",
    "_ = sns.swarmplot(x='Body mass (g)', y='', hue='label', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can observe that predicting the Gentoo penguins vs. the other species will be easy using the body mass. While the Adelie and Chinstrap species are difficult to differentiate. To simplify our illustration, we will therefore only select the Gentoo and Adelie species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_gentoo = y == \"Gentoo\"\n",
    "mask_adelie = y == \"Adelie\"\n",
    "X = X[mask_adelie | mask_gentoo]\n",
    "y = y[mask_adelie | mask_gentoo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Body mass (g)': X, 'label': y, '': \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clazz in np.unique(y):\n",
    "    plt.hist(X[y==clazz], alpha=0.7, label=f'{clazz}', density=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Body mass (g)')\n",
    "plt.ylabel('Class probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of some concepts: probabilities, entropy and information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees for classification are based on the following concepts: probabilities, entropy, and information gain. We present those concepts and describe where they come into play in the classification procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimate objective: find the best split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find a threshold which best split the dataset. In other words, we want the boundary such that most samples smaller than this boundary belong to a class and most samples above this boundary belong to the counterpart class. Let's visualise what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random state such we all have the same results\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = rng.choice(np.arange(y.size))\n",
    "\n",
    "ax = sns.swarmplot(x='Body mass (g)', y='', hue='label', data=df)\n",
    "ax.set_title(f\"random_idx={random_idx}\")\n",
    "_ = ax.plot([X[random_idx], X[random_idx]], [-1, 1], 'r-.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **Ctrl-Enter** to re-execute the previous cell several times in a row.\n",
    "\n",
    "\n",
    "In the above example, we expect as much as possible blue samples on the left of the decision boundary and as much as possible orange samples on the right of the decision boundary.\n",
    "\n",
    "Thus, we need to compute some statistics characteristic the quality of this split, sometimes called impurity. Let's group the labels into two groups, all labels corresponding to the samples on the left of the boundary and all labels corresponding to the samples on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>GET SOME INTUITIONS</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Which quantity could you compute to quantify the purity of the new datasets on the left and right side of the split?\n",
    "      </li>\n",
    "      <li>\n",
    "      How could you combine the parent, left, and right purity to have an overall indication of the split quality?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = 128  # restore the first random value to get deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(x='Body mass (g)', y='', hue='label', data=df)\n",
    "ax.set_title(f\"random_idx={random_idx}\")\n",
    "_ = ax.plot([X[random_idx], X[random_idx]], [-1, 1], 'r-.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_left = y[:random_idx]\n",
    "labels_right = y[random_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impurity is related to the number of samples of each class on a side of the boundary. We can compute the probability which is the number of samples of a class on the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(labels_left)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def compute_probability(labels):\n",
    "    counter = Counter(labels)\n",
    "    n_total_samples = sum(counter.values())\n",
    "    probabilities = {}\n",
    "    \n",
    "    for clazz, count_clazz in counter.items():\n",
    "        probabilities[clazz] = count_clazz / n_total_samples\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for the previous split, we could compute the probabilities for each class on the left and right side of the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_left = compute_probability(labels_left)\n",
    "print(f'Probability on the left split {prob_left}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_right = compute_probability(labels_right)\n",
    "print(f'Probability on the right split {prob_right}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results make sense. On the left side of the boundary, we mainly have Adelie penguin and very few Gentoo. On the right side of the partition, the separation is less ovious and the probability are not close to 0 and 1.\n",
    "\n",
    "Therefore, we can have the intuition that we need a split which will maximize the probability of a class on each side of the partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we computed probabilities. It gives several statistics depending on the class, and it will be handy to get a single statistic which characterises the impurity of a side of the boundary: we will use the entropy definition.\n",
    "\n",
    "Entropy is defined: $H(X) = - \\sum_{k=1}^{K} p(X_k) \\log_{2} p(X_k)$, where $K$ is the number of classes. In practise the entropy function looks like:\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/2/22/Binary_entropy_plot.svg)\n",
    "\n",
    "Considering some labels, the entropy will be maximum if there are as many samples from one class than another. It will be minimum when there are only samples from a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    probabilities = compute_probability(labels)\n",
    "\n",
    "    H_X = 0\n",
    "    for clazz in probabilities:\n",
    "        H_X -= probabilities[clazz] * np.log2(probabilities[clazz])\n",
    "\n",
    "    return H_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the entropy for each side of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_left = entropy(labels_left)\n",
    "print(f'Entropy on the left split {entropy_left}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_right = entropy(labels_right)\n",
    "print(f'Entropy on the right split {entropy_right}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      What would be the entropy of the full dataset given the fact that the two classes are balanced?\n",
    "      </li>\n",
    "      <li>\n",
    "      Check the correctness of your answer using the <tt>entropy</tt> function.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the entropy on the right side is small. We only had orange samples on that side. The entropy of the left side is closer to one because we have almost half-half orange and blue samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current split, we currently have two statistics, an entropy for each side of the split. It will be handy to get a single statistic to summarise the quality of the split. It is called the information gain.\n",
    "\n",
    "Indeed, the information gain corresponds to the entropy computed before splitting the data to which we subtract the entropy from the right and left side of the split (with some additional normalisation factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels_parent, labels_left, labels_right):\n",
    "    entropy_parent = entropy(labels_parent)\n",
    "    entropy_left = entropy(labels_left)\n",
    "    entropy_right = entropy(labels_right)\n",
    "\n",
    "    n_samples_parent = labels_parent.size\n",
    "    n_samples_left = labels_left.size\n",
    "    n_samples_right = labels_right.size\n",
    "\n",
    "    normalized_entropy_left = ((n_samples_left / n_samples_parent) * \n",
    "                               entropy_left)\n",
    "    normalized_entropy_right = ((n_samples_right / n_samples_parent) *\n",
    "                                entropy_right)\n",
    "\n",
    "    return (entropy_parent -\n",
    "            normalized_entropy_left - normalized_entropy_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain_split = information_gain(y, labels_left, labels_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Information for split at index {random_idx}:'\n",
    "      f' {info_gain_split:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaning of information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is called information gain because it relates to the gain in \"purity\" obtained by partitioning the data. We can try several random partitions and see that a higher is the gain corresponds to a better split to partition the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = rng.choice(np.arange(y.size))\n",
    "labels_left = y[:random_idx]\n",
    "labels_right = y[random_idx:]\n",
    "\n",
    "ax = sns.swarmplot(x='Body mass (g)', y='', hue='label', data=df)\n",
    "_ = ax.plot([X[random_idx], X[random_idx]], [-1, 1], '-.')\n",
    "\n",
    "info_gain_split = information_gain(y, labels_left, labels_right)\n",
    "print(f'Information for split at index {random_idx}:'\n",
    "      f' {info_gain_split:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press **Ctrl-Enter** to execute the previous cell several times in a row to see how the information gain evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Brute force search for the maximum information gain</b>:\n",
    "    <br>\n",
    "    Instead of trying to random split, we can use a greedy strategy by iteratively trying all possible value and use the one maximum the information gain.\n",
    "     <ul>\n",
    "      <li>\n",
    "      Iterate over all possible split indices in the dataset and store the information gain (note, the values in arrays X and y are already pre-sorted by increasing feature value in X);\n",
    "      </li>\n",
    "      <li>\n",
    "      Find the index corresponding to the maximum information gain;\n",
    "      </li>\n",
    "      <li>\n",
    "      Plot the all information gain values for all possible threshold;\n",
    "      </li>\n",
    "      <li>\n",
    "      Plot a marker for the threshold maximizing the information gain;\n",
    "      </li>\n",
    "      <li>\n",
    "      Plot the original swarm plot and superpose the line corresponding to the best threshold found.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: DecisionTreeClassifier as a decision stump</b>:\n",
    "    <ul>\n",
    "      <li>\n",
    "          Use a <tt>sklearn.tree.DecisionTreeClassifier</tt> to classifiy the data X and y. Train the decision tree. Ensure to use the good criterion and to limit the depth of the tree to get a similar process than what we did before.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Read the online documentation for [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to learn more about its constructor parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped = X[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO\n",
    "# tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this classifier is trained, we can plot the resulting decision and compare it with the previous procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_ = plot_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
